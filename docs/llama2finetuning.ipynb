{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-ml\n",
    "%pip install azure-identity\n",
    "%pip install datasets==2.9.0\n",
    "%pip install mlflow\n",
    "%pip install azureml-mlflow\n",
    "%pip install datasets==2.9.0\n",
    "%pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    ")\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential=credential)\n",
    "except:\n",
    "    workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "        resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "        workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    )\n",
    "\n",
    "# the models, fine tuning pipelines and environments are available in the AzureML system registry, \"azureml\"\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml\")\n",
    "registry_ml_client_meta = MLClient(credential, registry_name=\"azureml-meta\")\n",
    "\n",
    "experiment_name = \"text-generation-samsum\"\n",
    "\n",
    "# generating a unique timestamp that can be used for names and versions that need to be unique\n",
    "timestamp = str(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Llama-2-7b\"\n",
    "#model_name = \"Llama-2-70b\"\n",
    "#model_name = \"Llama-2-13b\"\n",
    "foundation_model = registry_ml_client_meta.models.get(model_name, label=\"latest\")\n",
    "print(\n",
    "    \"\\n\\nUsing model name: {0}, version: {1}, id: {2} for fine tuning\".format(\n",
    "        foundation_model.name, foundation_model.version, foundation_model.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "if \"computes_allow_list\" in foundation_model.tags:\n",
    "    computes_allow_list = ast.literal_eval(\n",
    "        foundation_model.tags[\"computes_allow_list\"]\n",
    "    )  # convert string to python list\n",
    "    computes_allow_list.append(\"Standard_NC48ads_A100_v4\")\n",
    "    computes_allow_list.append(\"Standard_NC96ads_A100_v4\") \n",
    "    print(f\"Please create a compute from the above list - {computes_allow_list}\")\n",
    "else:\n",
    "    computes_allow_list = None\n",
    "    print(\"Computes allow list is not part of model tags\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a specific compute size to work with change it here. By default we use the 8 x V100 compute from the above list\n",
    "compute_cluster_size = \"Standard_NC96ads_A100_v4\"\n",
    "\n",
    "# If you already have a gpu cluster, mention it here. Else will create a new one with the name 'gpu-cluster-big'\n",
    "#compute_cluster = \"gpu-cluster-big\"\n",
    "compute_cluster = \"gpu-cluster-large2\"\n",
    "\n",
    "try:\n",
    "    compute = workspace_ml_client.compute.get(compute_cluster)\n",
    "    print(\"The compute cluster already exists! Reusing it for the current run\")\n",
    "except Exception as ex:\n",
    "    print(\n",
    "        f\"Looks like the compute cluster doesn't exist. Creating a new one with compute size {compute_cluster_size}!\"\n",
    "    )\n",
    "    try:\n",
    "        print(\"Attempt #1 - Trying to create a dedicated compute\")\n",
    "        compute = AmlCompute(\n",
    "            name=compute_cluster,\n",
    "            size=compute_cluster_size,\n",
    "            tier=\"Dedicated\",\n",
    "            max_instances=2,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            print(\n",
    "                \"Attempt #2 - Trying to create a low priority compute. Since this is a low priority compute, the job could get pre-empted before completion.\"\n",
    "            )\n",
    "            compute = AmlCompute(\n",
    "                name=compute_cluster,\n",
    "                size=compute_cluster_size,\n",
    "                tier=\"LowPriority\",\n",
    "                max_instances=2,  # For multi node training set this to an integer value more than 1\n",
    "            )\n",
    "            workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise ValueError(\n",
    "                f\"WARNING! Compute size {compute_cluster_size} not available in workspace\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_count_found = False\n",
    "workspace_compute_sku_list = workspace_ml_client.compute.list_sizes()\n",
    "available_sku_sizes = []\n",
    "for compute_sku in workspace_compute_sku_list:\n",
    "    available_sku_sizes.append(compute_sku.name)\n",
    "    if compute_sku.name.lower() == compute.size.lower():\n",
    "        gpus_per_node = compute_sku.gpus\n",
    "        gpu_count_found = True\n",
    "        print(compute_sku.name.lower())\n",
    "# if gpu_count_found not found, then print an error\n",
    "if gpu_count_found:\n",
    "    print(f\"Number of GPU's in compute {compute.size}: {gpus_per_node}\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Number of GPU's in compute {compute.size} not found. Available skus are: {available_sku_sizes}.\"\n",
    "        f\"This should not happen. Please check the selected compute cluster: {compute_cluster} and try again.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "exit_status = os.system(\"python ./download-dataset.py --download_dir samsum-dataset\")\n",
    "if exit_status != 0:\n",
    "    raise Exception(\"Error downloading dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ./samsum-dataset/train.jsonl file into a pandas dataframe and show the first 5 rows\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", 0\n",
    ")  # set the max column width to 0 to display the full text\n",
    "df = pd.read_json(\"./samsum-dataset/train.jsonl\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to preprocess the dataset in desired format\n",
    "\n",
    "def get_preprocessed_samsum(df):\n",
    "    prompt = f\"Summarize this dialog:\\n{{}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "    df[\"text\"] = df[\"dialogue\"].map(prompt.format)\n",
    "    df = df.drop(columns=[\"dialogue\", \"id\"])\n",
    "    df = df[[\"text\", \"summary\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test.jsonl, train.jsonl and validation.jsonl form the ./samsum-dataset folder into pandas dataframes\n",
    "test_df = pd.read_json(\"./samsum-dataset/test.jsonl\", lines=True)\n",
    "train_df = pd.read_json(\"./samsum-dataset/train.jsonl\", lines=True)\n",
    "validation_df = pd.read_json(\"./samsum-dataset/validation.jsonl\", lines=True)\n",
    "# map the train, validation and test dataframes to preprocess function\n",
    "train_df = get_preprocessed_samsum(train_df)\n",
    "validation_df = get_preprocessed_samsum(validation_df)\n",
    "test_df = get_preprocessed_samsum(test_df)\n",
    "# show the first 5 rows of the train dataframe\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 10% of the rows from the train, validation and test dataframes into files with small_ prefix in the ./samsum-dataset folder\n",
    "frac = 1\n",
    "train_df.sample(frac=frac).to_json(\n",
    "    \"./samsum-dataset/small_train.jsonl\", orient=\"records\", lines=True\n",
    ")\n",
    "validation_df.sample(frac=frac).to_json(\n",
    "    \"./samsum-dataset/small_validation.jsonl\", orient=\"records\", lines=True\n",
    ")\n",
    "test_df.sample(frac=frac).to_json(\n",
    "    \"./samsum-dataset/small_test.jsonl\", orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "training_parameters = dict(\n",
    "    num_train_epochs=3,\n",
    "    #per_device_train_batch_size=32,\n",
    "    #per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "print(f\"The following training parameters are enabled - {training_parameters}\")\n",
    "\n",
    "# Optimization parameters - As these parameters are packaged with the model itself, lets retrieve those parameters\n",
    "if \"model_specific_defaults\" in foundation_model.tags:\n",
    "    optimization_parameters = ast.literal_eval(\n",
    "        foundation_model.tags[\"model_specific_defaults\"]\n",
    "    )  # convert string to python dict\n",
    "else:\n",
    "    optimization_parameters = dict(\n",
    "        apply_lora=\"true\", apply_deepspeed=\"true\", apply_ort=\"true\"\n",
    "    )\n",
    "print(f\"The following optimizations are enabled - {optimization_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import CommandComponent, PipelineComponent, Job, Component\n",
    "from azure.ai.ml import PyTorchDistribution, Input\n",
    "\n",
    "# fetch the pipeline component\n",
    "pipeline_component_func = registry_ml_client.components.get(\n",
    "    name=\"text_generation_pipeline\", label=\"latest\"\n",
    ")\n",
    "\n",
    "\n",
    "# define the pipeline job\n",
    "@pipeline()\n",
    "def create_pipeline():\n",
    "    text_generation_pipeline = pipeline_component_func(\n",
    "        # specify the foundation model available in the azureml system registry id identified in step #3\n",
    "        mlflow_model_path=foundation_model.id,\n",
    "        # huggingface_id = 'meta-llama/Llama-2-7b', # if you want to use a huggingface model, uncomment this line and comment the above line\n",
    "        compute_model_import=compute_cluster,\n",
    "        compute_preprocess=compute_cluster,\n",
    "        compute_finetune=compute_cluster,\n",
    "        compute_model_evaluation=compute_cluster,\n",
    "        # map the dataset splits to parameters\n",
    "        train_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./samsum-dataset/small_train.jsonl\"\n",
    "        ),\n",
    "        validation_file_path=Input(\n",
    "            type=\"uri_file\", path=\"./samsum-dataset/small_validation.jsonl\"\n",
    "        ),\n",
    "        test_file_path=Input(type=\"uri_file\", path=\"./samsum-dataset/small_test.jsonl\"),\n",
    "        evaluation_config=Input(type=\"uri_file\", path=\"./text-generation-config.json\"),\n",
    "        # The following parameters map to the dataset fields\n",
    "        text_key=\"text\",\n",
    "        ground_truth_key=\"summary\",\n",
    "        num_nodes_finetune=2,\n",
    "        # Training settings\n",
    "        number_of_gpu_to_use_finetuning=gpus_per_node,  # set to the number of GPUs available in the compute\n",
    "        **training_parameters,\n",
    "        **optimization_parameters\n",
    "    )\n",
    "    return {\n",
    "        # map the output of the fine tuning job to the output of pipeline job so that we can easily register the fine tuned model\n",
    "        # registering the model is required to deploy the model to an online or batch endpoint\n",
    "        \"trained_model\": text_generation_pipeline.outputs.mlflow_model_folder\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline_object = create_pipeline()\n",
    "\n",
    "# don't use cached results from previous jobs\n",
    "pipeline_object.settings.force_rerun = True\n",
    "\n",
    "# set continue on step failure to False\n",
    "pipeline_object.settings.continue_on_step_failure = False\n",
    "\n",
    "# set the pytorch and mlflow mode to mount\n",
    "\n",
    "pipeline_object.jobs[\"text_generation_pipeline\"][\"outputs\"][\"pytorch_model_folder\"].mode = \"mount\"\n",
    "\n",
    "pipeline_object.jobs[\"text_generation_pipeline\"][\"outputs\"][\"mlflow_model_folder\"].mode = \"mount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = workspace_ml_client.jobs.create_or_update(\n",
    "    pipeline_object, experiment_name=experiment_name\n",
    ")\n",
    "# wait for the pipeline job to complete\n",
    "workspace_ml_client.jobs.stream(pipeline_job.name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
